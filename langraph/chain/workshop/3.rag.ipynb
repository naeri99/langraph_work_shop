{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a6b527-1cc2-4a93-ab59-cc9397a647d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import boto3\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from langchain_core.documents import Document\n",
    "import fitz\n",
    "import concurrent.futures\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d2b643-f73a-4261-ab5c-5d968c54c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # replace with your region\n",
    ")\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",  # You can choose other models like \"cohere.embed-english-v3\"\n",
    "    region_name=\"us-east-1\",  # Change to your AWS region\n",
    "    client=bedrock_client  # Optional: provide your own boto3 client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7712d261-d386-4f43-87b2-c6716dc68b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text를 vector로 변환\n",
    "def get_embedding(text):\n",
    "    session = boto3.Session()\n",
    "    bedrock = session.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name='us-east-1'  # Specify a region\n",
    "    )\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=json.dumps({\"inputText\": text}),\n",
    "        modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return response_body['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c69f1e-8e6d-4bff-9df0-f5cbc481c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opensearch에 index(sql table과 유사) 생성\n",
    "def define_index(opensearch_client, index_name):\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"index\": {  \n",
    "                \"knn\": True,                          \n",
    "                \"knn.algo_param.ef_search\": 100,      \n",
    "                \"number_of_shards\": 3,                \n",
    "                \"number_of_replicas\": 2, \n",
    "                \"analysis\": {\n",
    "                    \"analyzer\": {\n",
    "                        \"nori_analyzer\": {\n",
    "                            \"tokenizer\": \"nori_tokenizer\",\n",
    "                            \"filter\": [\"nori_stop\", \"lowercase\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"filter\": {\n",
    "                        \"nori_stop\": {\n",
    "                            \"type\": \"nori_part_of_speech\",\n",
    "                            \"stoptags\": [\"J\", \"JKS\", \"JKB\", \"JKO\", \"JKG\", \"JKC\", \"JKV\", \"JKQ\", \"JX\", \"JC\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },   \n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"document\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"nori_analyzer\"\n",
    "                },\n",
    "                \"doc_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 1024,\n",
    "                    \"method\": {\n",
    "                        \"name\": \"hnsw\",   \n",
    "                        \"space_type\": \"l2\",\n",
    "                        \"engine\": \"faiss\",\n",
    "                        \"parameters\": {\n",
    "                            \"ef_construction\": 128,\n",
    "                            \"m\": 16\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"source\": {\n",
    "                    \"type\": \"keyword\"  # Use keyword for exact matching\n",
    "                },\n",
    "                \"page\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"total_pages\": {\n",
    "                    \"type\": \"integer\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = opensearch_client.indices.create(\n",
    "            index=index_name,\n",
    "            body=index_settings\n",
    "        )\n",
    "        print(\"Index created successfully:\", response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error creating index:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20520f7-34e9-4aea-88fb-59b55c45ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf파일 opensearch에 적재\n",
    "def save_chunk(opensearh_client,  index_name,chunks):\n",
    "    for d in enumerate(chunks):\n",
    "        document = d[1].page_content\n",
    "        raw_embedding = bedrock_embeddings.embed_query(d[1].page_content)\n",
    "        doc_vector = [float(val) for val in raw_embedding]\n",
    "        source = d[1].metadata[\"source\"]\n",
    "        page_info= d[1].metadata[\"page\"]\n",
    "        total_page=d[1].metadata[\"total_pages\"]\n",
    "    \n",
    "        # Prepare document for indexing\n",
    "        document = {\n",
    "            \"document\": document,\n",
    "            \"doc_vector\":doc_vector,\n",
    "            \"source\" : source, \n",
    "            \"page\" : page_info , \n",
    "            \"total_pages\" : total_page \n",
    "        }\n",
    "        try:\n",
    "            response = opensearh_client.index(\n",
    "                index=index_name,\n",
    "                body=document,\n",
    "                id=f\"sagemaker_doc_{d[0]}\"\n",
    "            )\n",
    "            if d[0] % 500 ==0 :\n",
    "                print(f\"Document {d[0]} indexed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing document {d[0]}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74069b2-4be4-4fb4-b5a8-b3cf58efb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#속도를 위해 parallel하게 진행. 페이지를 부분적으로 나누어서 text파일 읽기\n",
    "def process_page_range(pdf_path, start_page, end_page):\n",
    "    \"\"\"Process a range of pages from a PDF.\"\"\"\n",
    "    # Open the PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    \n",
    "    batch_docs = []\n",
    "    for i in range(start_page, end_page):\n",
    "        if i >= total_pages:\n",
    "            break\n",
    "            \n",
    "        # Get page and extract text\n",
    "        page = doc[i]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Create Document object\n",
    "        doc_obj = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": i + 1,\n",
    "                \"total_pages\": total_pages\n",
    "            }\n",
    "        )\n",
    "        batch_docs.append(doc_obj)\n",
    "    \n",
    "    return batch_docs\n",
    "    \n",
    "#pdf파일 읽기 \n",
    "def load_pdf_parallel(pdf_path, batch_size=100, max_workers=4):\n",
    "    \"\"\"Load a PDF using parallel processing.\"\"\"\n",
    "    # Get total pages\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    doc.close()\n",
    "    print(f\"PDF has {total_pages} pages in total\")\n",
    "    \n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for start_page in range(0, total_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, total_pages)\n",
    "        batches.append((start_page, end_page))\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_batch = {\n",
    "            executor.submit(process_page_range, pdf_path, start, end): (start, end)\n",
    "            for start, end in batches\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            start, end = future_to_batch[future]\n",
    "            try:\n",
    "                batch_docs = future.result()\n",
    "                all_docs.extend(batch_docs)\n",
    "                print(f\"Completed batch: pages {start+1} to {end}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pages {start+1} to {end}: {e}\")\n",
    "    \n",
    "    # Sort by page number\n",
    "    all_docs.sort(key=lambda x: x.metadata[\"page\"])\n",
    "    \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "110a12f5-b02e-4170-94a1-b90e793889f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection failed: ConnectionError(HTTPSConnectionPool(host='opensearch_url', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f902060cd10>: Failed to resolve 'opensearch_url' ([Errno -2] Name or service not known)\"))) caused by: ConnectionError(HTTPSConnectionPool(host='opensearch_url', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f902060cd10>: Failed to resolve 'opensearch_url' ([Errno -2] Name or service not known)\")))\n"
     ]
    }
   ],
   "source": [
    "#opensearh client 연결 생성 및 확인\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts = [{'host': 'opensearch_url', 'port': 443}],\n",
    "    http_auth = ('user', 'password'),  # Replace with your master credentials\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "try:\n",
    "    response = opensearch_client.info()\n",
    "    print(\"Successfully connected to OpenSearch\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6fcd42-95f0-48ef-ae4b-fc53d297db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 2716 pages in total\n",
      "Completed batch: pages 101 to 200\n",
      "Completed batch: pages 1 to 100\n",
      "Completed batch: pages 201 to 300\n",
      "Completed batch: pages 301 to 400\n",
      "Completed batch: pages 401 to 500\n",
      "Completed batch: pages 501 to 600\n",
      "Completed batch: pages 601 to 700\n",
      "Completed batch: pages 701 to 800\n",
      "Completed batch: pages 801 to 900\n",
      "Completed batch: pages 901 to 1000\n",
      "Completed batch: pages 1001 to 1100\n",
      "Completed batch: pages 1201 to 1300\n",
      "Completed batch: pages 1101 to 1200\n",
      "Completed batch: pages 1301 to 1400\n",
      "Completed batch: pages 1401 to 1500\n",
      "Completed batch: pages 1501 to 1600\n",
      "Completed batch: pages 1601 to 1700\n",
      "Completed batch: pages 1701 to 1800\n",
      "Completed batch: pages 1801 to 1900\n",
      "Completed batch: pages 1901 to 2000\n",
      "Completed batch: pages 2001 to 2100\n",
      "Completed batch: pages 2201 to 2300\n",
      "Completed batch: pages 2101 to 2200\n",
      "Completed batch: pages 2301 to 2400\n",
      "Completed batch: pages 2401 to 2500\n",
      "Completed batch: pages 2601 to 2700\n",
      "Completed batch: pages 2501 to 2600\n",
      "Completed batch: pages 2701 to 2716\n"
     ]
    }
   ],
   "source": [
    "#pdf파일을 chunk로 나눈다\n",
    "path=\"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\"\n",
    "page=load_pdf_parallel(path,max_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61325a0-2acb-4546-99fb-51d8d841767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#반환된 결과는 List[Document] 형태입니다.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbf2f30-d2eb-464d-b566-b615a15d8cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 갯수 ->  7468\n"
     ]
    }
   ],
   "source": [
    "print(\"총 갯수 -> \",len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b75f974-2a47-4ad5-850c-e3c5209ccd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 's3_explain' deleted successfully: {'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    delete_response = opensearch_client.indices.delete(index='s3_explain')\n",
    "    print(f\"Index 's3_explain' deleted successfully: {delete_response}\")\n",
    "except Exception as delete_error:\n",
    "    print(f\"Failed to delete index 's3_explain': {delete_error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b9e93b2-0c34-4530-8788-13581dbc4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created successfully: {'acknowledged': True, 'shards_acknowledged': True, 'index': 's3_explain'}\n"
     ]
    }
   ],
   "source": [
    "#index 생성\n",
    "define_index(opensearch_client, \"s3_explain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5580d4db-458a-44e1-bd85-d28cf617395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 indexed successfully\n",
      "Document 500 indexed successfully\n",
      "Document 1000 indexed successfully\n",
      "Document 1500 indexed successfully\n",
      "Document 2000 indexed successfully\n",
      "Document 2500 indexed successfully\n",
      "Document 3000 indexed successfully\n",
      "Document 3500 indexed successfully\n",
      "Document 4000 indexed successfully\n",
      "Document 4500 indexed successfully\n",
      "Document 5000 indexed successfully\n",
      "Document 5500 indexed successfully\n",
      "Document 6000 indexed successfully\n",
      "Document 6500 indexed successfully\n",
      "Document 7000 indexed successfully\n"
     ]
    }
   ],
   "source": [
    "save_chunk(opensearch_client, \"s3_explain\" , split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af5be344-c02e-49d9-aa2b-3391c2afd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BM25 search ( nori_analyzer를 이용하여 형태소 분석을 진행한후 텍스트 유사도 측정 )\n",
    "def simple_text_search(client, search_text, index_name=\"s3_explain\", k=10):\n",
    "    try:\n",
    "        # Simple text search query\n",
    "        text_query = {\n",
    "            \"size\": k,  # Number of results to return\n",
    "            \"_source\": {\n",
    "                \"excludes\": [\"doc_vector\"]  # Exclude vector field from results\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"document\": {\n",
    "                        \"query\": search_text,\n",
    "                        \"analyzer\": \"nori_analyzer\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute search\n",
    "        response = client.search(\n",
    "            index=index_name,\n",
    "            body=text_query\n",
    "        )\n",
    "\n",
    "\n",
    "        documents = []\n",
    "        if response.get(\"hits\", {}).get(\"hits\", []):\n",
    "            search_results = normalize_search_results(response) \n",
    "            for res in search_results[\"hits\"][\"hits\"]:\n",
    "                source = res['_source']\n",
    "                page_content = {k: source[k] for k in source if k != \"table_summary_v\"}\n",
    "                metadata = {\"id\": res['_id']}\n",
    "                score = res['_score']  # Get the score from the search result\n",
    "                documents.append((Document(page_content=json.dumps(page_content, ensure_ascii=False), metadata=metadata), score))\n",
    "        return documents  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "#BM25 search의 score는 0.1 to 15.0 임으로 normazlie(0~1)를 해야함 (추후 vector search와 score 비교를 하기위해)\n",
    "def normalize_search_results(search_results):\n",
    "        hits = (search_results[\"hits\"][\"hits\"])\n",
    "        max_score = float(search_results[\"hits\"][\"max_score\"])\n",
    "        for hit in hits:\n",
    "            hit[\"_score\"] = float(hit[\"_score\"]) / max_score\n",
    "        search_results[\"hits\"][\"max_score\"] = hits[0][\"_score\"]\n",
    "        search_results[\"hits\"][\"hits\"] = hits\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "429d1369-34e0-4602-84f8-a3d69c4945b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector 유사도 검색 \n",
    "def vector_search(client, embedding_vector, index_name=\"s3_explain\", k=3):\n",
    "    try:\n",
    "        # KNN vector search query\n",
    "        vector_query = {\n",
    "            \"size\": k,\n",
    "            \"_source\": {\n",
    "                \"excludes\": [\"doc_vector\"]  # Exclude vector field from results\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"doc_vector\": {\n",
    "                        \"vector\": embedding_vector,\n",
    "                        \"k\": k\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute the search\n",
    "        response = client.search(\n",
    "            index=index_name,\n",
    "            body=vector_query\n",
    "        )\n",
    "\n",
    "        documents = []\n",
    "        for res in response[\"hits\"][\"hits\"]:\n",
    "            source = res['_source']\n",
    "            page_content = {k: source[k] for k in source if k != \"vector\"}\n",
    "            metadata = {\"id\": res['_id']}  # Add metadata with a unique identifier\n",
    "            score = res['_score']  # Get the match score from the search result\n",
    "            documents.append((Document(page_content=json.dumps(page_content, ensure_ascii=False), metadata=metadata), score))\n",
    "        return documents\n",
    "            \n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60505c6-e3d4-4144-9f2f-e21fb6e0de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#노리를 통해 검색한 10개 + vector 유사도 기반으로 검색한 10개 총 20개를 정렬\n",
    "def get_ensemble_results(doc_lists: List[List[Tuple[Document, float]]], weights: List[float], k: int = 5) -> List[Document]:\n",
    "        hybrid_score_dic: Dict[str, float] = {}\n",
    "        doc_map: Dict[str, Document] = {}\n",
    "        \n",
    "        # Weight-based adjustment\n",
    "        for doc_list, weight in zip(doc_lists, weights):\n",
    "            for doc, score in doc_list:\n",
    "                doc_id = doc.metadata.get(\"id\", doc.page_content)\n",
    "                if doc_id not in hybrid_score_dic:\n",
    "                    hybrid_score_dic[doc_id] = 0.0\n",
    "                hybrid_score_dic[doc_id] += score * weight\n",
    "                doc_map[doc_id] = doc\n",
    "    \n",
    "        sorted_docs = sorted(hybrid_score_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [doc_map[doc_id] for doc_id, _ in sorted_docs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa43228-9cce-48f4-b860-c807c5e07b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이브리드 서치 코드 -> 노리를 통해 검색한 10개 + vector 유사도 기반으로 검색한 10개 총 20개에서 rerank를 통해 최종 10개만 선택한다\n",
    "def retrieval_augmented(query , k =10): \n",
    "    \n",
    "    #embedding \n",
    "    raw_embedding = get_embedding(query)\n",
    "    #vector search 가져오기 \n",
    "    vector=vector_search(opensearch_client, raw_embedding ,k =k)\n",
    "    #lexical search 가져오기 \n",
    "    lexical=simple_text_search(opensearch_client, query ,k =k)\n",
    "\n",
    "    #rerank\n",
    "    rerank_doc = get_ensemble_results(\n",
    "            doc_lists=[vector, lexical],\n",
    "            weights= [0.7, 0.3],\n",
    "            k=k,\n",
    "    )\n",
    "    return rerank_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d4d8b4-ac1a-4500-b6ca-6afc6c0f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과 검색\n",
    "result =retrieval_augmented(\"Amazon S3 glacier에 대해 설명해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffc017b8-af7d-4708-ba09-47f48967e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'sagemaker_doc_5760'}, page_content='{\"document\": \"Amazon Simple Storage Service\\\\n사용 설명서\\\\nGlacier Instant Retrieval, S3 Glacier Flexible Retrieval 또는 Reduced Redundancy Storage를 스토\\\\n리지 클래스로 지정하여 객체를 덮어씁니다.\\\\nNote\\\\n복원된 객체에 대한 복사 작업은 Amazon S3 콘솔에서 S3 Glacier Flexible Retrieval 또는 S3 \\\\nGlacier Deep Archive 스토리지 클래스에 있는 객체에 대해 지원되지 않습니다. 이러한 유형\\\\n의 복사 작업에는\\xa0AWS Command Line Interface(AWS CLI),\\xa0AWS\\xa0SDK 또는 REST API를 \\\\n사용하십시오.\\\\nS3 Glacier Flexible Retrieval 및 S3 Glacier Deep Archive 스토리지 클래스에 저장된 객체는 \\\\nAmazon S3를 통해서만 확인하고 사용할 수 있습니다. 별도의 Amazon S3 Glacier 서비스를 통해 사\\\\n용할 수는 없습니다.\\\\n이러한 객체는 Amazon S3 객체이므로 Amazon S3 콘솔 또는 Amazon S3 API를 통해서만 액세스할\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 2074, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_5702'}, page_content='{\"document\": \"Amazon Simple Storage Service\\\\n사용 설명서\\\\nImportant\\\\n모든 장기 데이터에 대해 Amazon S3 서비스 내의 S3 Glacier 스토리지 클래스를 사용하는 것\\\\n이 좋습니다.\\\\nAmazon S3 Glacier(S3 Glacier) 서비스는 볼트에 데이터를 아카이브로 저장하는 별도의 서비스입니\\\\n다. 이 서비스는 Amazon S3 기능을 지원하지 않으며 데이터 업로드 및 다운로드 작업에 콘솔 지원을 \\\\n제공하지 않습니다. 장기 데이터에는 S3 Glacier 서비스를 사용하지 않는 것이 좋습니다. 이 서비스에 \\\\n저장된 데이터는 Amazon S3 서비스에서 액세스할 수 없습니다. S3 Glacier 서비스에 대한 자세한 내\\\\n용은 Amazon S3 Glacier 개발자 가이드를 참조하세요. Amazon S3 Glacier 서비스에서 Amazon S3의 \\\\n스토리지 클래스로 데이터를 전송하려면 AWS 솔루션 라이브러리의 Data Transfer from Amazon S3 \\\\nGlacier Vaults to Amazon S3를 참조하세요.\\\\n아카이브된 객체 작업\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 2054, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_5610'}, page_content='{\"document\": \"Amazon Simple Storage Service\\\\n사용 설명서\\\\n• S3 Glacier Instant Retrieval(GLACIER_IR) – 거의 액세스하지 않고 밀리초 단위로 검색해야 하는 장\\\\n기 데이터에 사용합니다. 이 스토리지 클래스의 데이터는 실시간 액세스에 사용할 수 있습니다.\\\\n• S3 Glacier Flexible Retrieval(GLACIER) – 분 단위로 데이터의 일부를 검색해야 하는 아카이브에 사\\\\n용합니다. 이 스토리지 클래스의 데이터는 아카이빙되며 실시간 액세스에 사용할 수 없습니다.\\\\n• S3 Glacier Deep Archive(DEEP_ARCHIVE) – 거의 액세스할 필요가 없는 데이터를 아카이브할 때 사\\\\n용합니다. 이 스토리지 클래스의 데이터는 아카이빙되며 실시간 액세스에 사용할 수 없습니다.\\\\n아카이브된 객체 복원\\\\n객체의 스토리지 클래스 설정 섹션에서 설명한 것처럼 기타 스토리지 클래스와 동일한 방법으\\\\n로 객체의 스토리지 클래스를 S3 Glacier Flexible Retrieval(GLACIER) 또는 S3 Glacier Deep\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 2023, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_5684'}, page_content='{\"document\": \"Amazon Simple Storage Service\\\\n사용 설명서\\\\n장기 데이터 스토리지를 위한 S3 Glacier 스토리지 클래스 이해하기\\\\nAmazon S3 S3 Glacier 스토리지 클래스를 사용하여 자주 액세스하지 않는 장기 데이터를 저장하는 비\\\\n용 효율적인 솔루션을 제공할 수 있습니다. S3 Glacier 스토리지 클래스는 다음과 같습니다.\\\\n• S3 Glacier Instant Retrieval\\\\n• S3 Glacier Flexible Retrieval\\\\n• S3 Glacier Deep Archive\\\\n데이터에 액세스하는 빈도와 요구되는 데이터 검색 속도에 따라 이러한 스토리지 클래스 중 하나를 \\\\n선택합니다. 각 스토리지 클래스는 S3 Standard 스토리지 클래스와 동일한 내구성 및 복원성을 제\\\\n공하지만 스토리지 비용은 더 낮습니다. S3 Glacier 스토리지 클래스에 대한 자세한 내용은 https:// \\\\naws.amazon.com/s3/storage-classes/glacier/를 참조하세요.\\\\n주제\\\\n• S3 Glacier 스토리지 클래스 비교\\\\n• S3 Glacier Instant Retrieval\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 2049, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_2'}, page_content='{\"document\": \"Amazon Simple Storage Service\\\\n사용 설명서\\\\nTable of Contents\\\\nAmazon S3란 무엇인가요? .................................................................................................................. 1\\\\nAmazon S3의 기능.......................................................................................................................... 1\\\\n스토리지 클래스.......................................................................................................................... 1\\\\n스토리지 관리............................................................................................................................. 2\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 3, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_146'}, page_content='{\"document\": \"Amazon Simple Storage Service\\\\n사용 설명서\\\\nAmazon S3란 무엇인가요?\\\\nAmazon Simple Storage Service(Amazon S3)는 업계 최고의 확장성, 데이터 가용성, 보안 및 성능을 \\\\n제공하는 객체 스토리지 서비스입니다. 모든 규모와 업종의 고객은 Amazon S3를 사용하여 데이터 레\\\\n이크, 웹 사이트, 모바일 애플리케이션, 백업 및 복원, 아카이브, 엔터프라이즈 애플리케이션, IoT 디바\\\\n이스, 빅 데이터 분석 등 다양한 사용 사례에서 원하는 양의 데이터를 저장하고 보호할 수 있습니다. \\\\nAmazon S3는 특정 비즈니스, 조직 및 규정 준수 요구 사항에 맞게 데이터에 대한 액세스를 최적화, 구\\\\n조화 및 구성할 수 있는 관리 기능을 제공합니다.\\\\nNote\\\\nAmazon S3 Express One Zone 스토리지 클래스를 디렉터리 버킷과 함께 사용하는 방법에 대\\\\n한 자세한 내용은 S3 Express One Zone 및 디렉터리 버킷 작업 섹션을 참조하세요.\\\\n주제\\\\n• Amazon S3의 기능\\\\n• Amazon S3 작동 방식\\\\n• Amazon S3 데이터 일관성 모델\\\\n• 관련 서비스\\\\n• Amazon S3 액세스\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 19, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_116'}, page_content='{\"document\": \"Amazon S3 Intelligent-Tiering을 사용한 스토리지 비용 관리............................................... 2018\\\\nAmazon S3 Glacier 스토리지 클래스................................................................................... 2031\\\\n아카이브된 객체 작업........................................................................................................... 2036\\\\n수명 주기 관리........................................................................................................................... 2048\\\\n객체의 전체 수명 주기 관리................................................................................................. 2050\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 15, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_5608'}, page_content='{\"document\": \"사용합니다.\\\\n• S3 One Zone-IA(ONEZONE_IA) – 가용 영역에 장애 발생 시 데이터를 다시 생성할 수 있는 경우와 \\\\nS3 교차 리전 복제(CRR)를 구성하는 경우 객체 복제본에 사용합니다. 또한 데이터 레지던시 및 격리\\\\n의 경우 AWS 로컬 영역에서 디렉터리 버킷을 생성하고 S3 One Zone-IA 스토리지 클래스를 사용할 \\\\n수 있습니다.\\\\n거의 액세스하지 않는 객체를 위한 스토리지 클래스\\\\nS3 Glacier Instant Retrieval(GLACIER_IR), S3 Glacier Flexible Retrieval(GLACIER) 및 S3 Glacier \\\\nDeep Archive(DEEP_ARCHIVE) 스토리지 클래스는 저비용의 장기 데이터 저장 및 데이터 아카이브를 \\\\n위해 설계되었습니다. 이러한 스토리지 클래스는 최소 스토리지 기간과 검색 요금이 필요하므로, 거의 \\\\n액세스하지 않는 데이터에 가장 효과적입니다. S3 Glacier 스토리지 클래스에 대한 자세한 정보는 장기 \\\\n데이터 스토리지를 위한 S3 Glacier 스토리지 클래스 이해하기 섹션을 참조하세요.\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 2022, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_4719'}, page_content='{\"document\": \"Amazon Simple Storage Service\\\\n사용 설명서\\\\n스토리지 클래스\\\\nAmazon S3는 워크로드 요구 사항에 따라 선택할 수 있는 다양한 스토리지 클래스를 제공합니다. \\\\nS3 Standard-IA 및 S3 One Zone-IA 스토리지 클래스는 한 달에 한 번 정도 액세스하며 밀리초 단\\\\n위의 액세스가 필요한 데이터용으로 설계되었습니다. S3 Glacier Instant Retrieval 스토리지 클래\\\\n스는 분기에 한 번 정도 액세스하며 밀리초 단위의 액세스가 필요한 수명이 긴 아카이브 데이터용\\\\n으로 설계되었습니다. 백업과 같이 즉각적인 액세스가 필요하지 않은 아카이브 데이터의 경우 S3 \\\\nGlacier Flexible Retrieval 또는 S3 Glacier Deep Archive 스토리지 클래스를 사용할 수 있습니다. \\\\n자세한 내용은 Amazon S3 스토리지 클래스 이해 및 관리 섹션을 참조하세요.\\\\n다음 보안 모범 사례에서도 복원성을 다룹니다.\\\\n• Enable versioning\\\\n• Consider Amazon S3 cross-region replication\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 1683, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_6200'}, page_content='{\"document\": \"Storage , DeepArchiveStorage\\\\n• S3 Glacier Flexible Retrieval: GlacierObjectOverhead\\\\n,\\\\nGlacierS3ObjectOverhead\\\\n, GlacierStagingStorage\\\\n,\\\\nGlacierStorage\\\\n• S3 Glacier Instant Retrieval: GlacierInstantRetrievalStor \\\\nage , GlacierIRSizeOverhead\\\\n• S3 Intelligent-Tiering: IntelligentTieringAAStorage\\\\n,\\\\nIntelligentTieringAIAStorage\\\\n, IntelligentTiering \\\\nDAAStorage\\\\n, IntelligentTieringFAStorage\\\\n,\\\\nIntelligentTieringIAStorage\\\\n• S3 One Zone-Infrequent Access: OneZoneIASizeOverhead\\\\n,\\\\nOneZoneIAStorage\\\\n• S3 Standard: StandardStorage\\\\n지표 및 차원\\\\nAPI 버전 2006-03-01 2217\", \"source\": \"/home/jovyan/langtest/bigdata/data/s3-userguide.pdf\", \"page\": 2235, \"total_pages\": 2716}')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491dfd8c-e9f4-479a-8c1b-bc7ef5c72d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc94909f-4535-49cf-82fe-083e7c47ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # replace with your region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9347a66-e62a-4dcd-bbde-513004035f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=bedrock_client,\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20d93c87-f36b-4cc0-8f06-cd726a08e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 생성\n",
    "template_lambda = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "{document}\n",
    "\n",
    "Conversation:\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_lambda = PromptTemplate(\n",
    "    input_variables=[\"document\", \"question\"], \n",
    "    template=template_lambda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5679158e-dfa8-417d-9628-7171ccd6063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input을 넣을때 함수 실행 (lambda 실행)\n",
    "chain_lambda_rag = (\n",
    "    {\n",
    "        \"document\": lambda x: retrieval_augmented(x[\"question\"]),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt_lambda\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1856c033-3854-483c-b251-540de28f8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_lambda_rag.invoke({\"question\": \"Amazon S3 glacier에 대해 설명해줘\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "353e335d-32ce-4f4c-a6e1-89ce7bd0b689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon S3 Glacier는 장기 데이터 저장 및 데이터 아카이브를 위한 저비용 스토리지 클래스입니다. S3 Glacier에는 세 가지 스토리지 클래스가 있습니다.\\n\\n1. S3 Glacier Instant Retrieval: 거의 액세스하지 않지만 밀리초 단위로 검색이 필요한 장기 데이터에 사용합니다. 실시간 액세스가 가능합니다.\\n\\n2. S3 Glacier Flexible Retrieval: 분 단위로 데이터의 일부를 검색해야 하는 아카이브에 사용합니다. 이 스토리지 클래스의 데이터는 아카이빙되며 실시간 액세스가 불가능합니다.\\n\\n3. S3 Glacier Deep Archive: 거의 액세스할 필요가 없는 데이터를 아카이브할 때 사용합니다. 이 스토리지 클래스의 데이터 또한 아카이빙되며 실시간 액세스가 불가능합니다.\\n\\nS3 Glacier 스토리지 클래스는 S3 Standard 스토리지 클래스와 동일한 내구성과 복원성을 제공하지만, 스토리지 비용이 더 낮습니다. S3 Glacier에 저장된 객체는 Amazon S3를 통해서만 액세스할 수 있으며, 별도의 Amazon S3 Glacier 서비스에서는 액세스할 수 없습니다.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d73de3-b4d5-4a8c-bb08-d5af9a4e36f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f8d1fed-711f-4cc0-af41-636a74e3844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RunnableLambda안에서 실행할 함수\n",
    "def get_document_runnable(data):\n",
    "    print(data)\n",
    "    # Make sure this function returns a string\n",
    "    return retrieval_augmented(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d180b6-b768-4042-9fe5-ce27d4eda918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input을 넣을때 함수 실행 (RunnableLambda 실행)\n",
    "chain_runnable_rag = (\n",
    "    {\n",
    "        \"document\": itemgetter(\"question\") | RunnableLambda(get_document_runnable),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt_lambda\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d6325c2-8dde-4d34-8063-24d5298ac9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon S3 glacier에 대해 설명해줘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Amazon S3 Glacier는 장기 데이터 보관을 위한 저비용 스토리지 클래스입니다. S3 Glacier에는 세 가지 스토리지 클래스가 있습니다.\\n\\n1. S3 Glacier Instant Retrieval: 거의 액세스하지 않고 밀리초 단위로 검색해야 하는 장기 데이터에 사용합니다. 이 클래스의 데이터는 실시간으로 액세스할 수 있습니다.\\n\\n2. S3 Glacier Flexible Retrieval: 분 단위로 데이터의 일부를 검색해야 하는 아카이브에 사용합니다. 이 클래스의 데이터는 아카이빙되어 실시간 액세스가 불가능합니다.\\n\\n3. S3 Glacier Deep Archive: 거의 액세스할 필요가 없는 데이터를 아카이브할 때 사용합니다. 이 클래스 또한 아카이빙되어 실시간 액세스가 불가능합니다.\\n\\nS3 Glacier 스토리지 클래스는 S3 Standard와 동일한 내구성과 복원성을 제공하지만, 스토리지 비용이 더 낮습니다. 데이터 액세스 빈도와 필요한 검색 속도에 따라 적절한 클래스를 선택하면 됩니다.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_runnable_rag.invoke({\"question\": \"Amazon S3 glacier에 대해 설명해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7ca22-f121-4bd5-b7c3-d65324a41741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fdc00-0eb7-4d8b-a5ab-521c366fae95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8a44858-10f7-4593-8403-1e9f6f5c1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nested llm chain\n",
    "def get_document_runnable_nested(data):\n",
    "    print(data)\n",
    "    # Make sure this function returns a string\n",
    "    return { \n",
    "             \"question\" : data,\n",
    "             \"document\": retrieval_augmented(data)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35d64821-957a-4fb5-be40-06b468f377f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_rerank = \"\"\"You are an AI document processor that organizes and summarizes information.\n",
    "\n",
    "TASK:\n",
    "1. Analyze the user's question to understand what information is relevant\n",
    "2. Review the provided document and identify sections that relate to the question\n",
    "3. Extract ONLY the parts of the document that are relevant to the question\n",
    "4. Organize these relevant parts into a clear, concise summary\n",
    "5. Remove any redundant or irrelevant information\n",
    "6. Format the output as a structured summary of key points\n",
    "7. DO NOT answer the question or provide any additional information not in the document\n",
    "8. DO NOT include your own analysis or opinions\n",
    "\n",
    "Document to Process:\n",
    "{document}\n",
    "\n",
    "Question for Context (use only to determine relevance):\n",
    "{question}\n",
    "\n",
    "OUTPUT INSTRUCTIONS:\n",
    "- Return ONLY the organized and summarized relevant information\n",
    "- Do not include any introduction, explanation, or conclusion\n",
    "- Do not address the question directly\n",
    "- Format as bullet points or short paragraphs of key information\n",
    "\n",
    "Organized Relevant Information:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_rerank = PromptTemplate(\n",
    "    input_variables=[\"document\", \"question\"], \n",
    "    template=template_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "181ee08c-a0ca-4d1f-84d8-2a1b86d21360",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_chain = (\n",
    "    {\n",
    "        \"document\": (\n",
    "                    itemgetter(\"question\") \n",
    "                    | RunnableLambda(get_document_runnable_nested) \n",
    "                    | prompt_rerank\n",
    "                    | llm\n",
    "                    | StrOutputParser()\n",
    "                    ),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt_lambda\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8151d1d6-8777-4b9a-8a81-66cf7e80b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon S3 glacier에 대해 설명해줘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Amazon S3 Glacier는 AWS의 저렴한 클라우드 스토리지 서비스입니다. 장기 데이터 보관 및 아카이브에 최적화되어 있습니다. 주요 특징은 다음과 같습니다:\\n\\n- S3 Glacier에는 세 가지 스토리지 클래스가 있습니다:\\n\\n1) S3 Glacier Instant Retrieval - 밀리초 단위로 데이터에 실시간 액세스가 가능합니다.\\n\\n2) S3 Glacier Flexible Retrieval - 분 단위로 일부 데이터 검색이 가능한 아카이브 데이터용입니다.\\n\\n3) S3 Glacier Deep Archive - 거의 액세스할 필요가 없는 아카이브 데이터용입니다.\\n\\n- 이러한 S3 Glacier 클래스들은 S3 Standard와 동일한 내구성과 복원성을 제공하지만, 스토리지 비용이 더 저렴합니다.\\n\\n- S3 Glacier의 데이터는 Amazon S3를 통해서만 액세스할 수 있습니다.\\n\\nS3 Glacier는 장기 보관이 필요한 데이터에 적합하며, 데이터 액세스 요구 사항에 따라 적절한 스토리지 클래스를 선택할 수 있습니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_chain.invoke({\"question\": \"Amazon S3 glacier에 대해 설명해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c099b5-0fed-4b8f-a76c-f06622ecba08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e87b4-883b-4f7c-b2c5-57654df88cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d178562-2351-4988-9ccd-c1b183663fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
